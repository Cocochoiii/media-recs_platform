# ğŸš€ Media Recommender System - è¿è¡ŒæŒ‡å—

## ç›®å½•
1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [ç¯å¢ƒè®¾ç½®](#ç¯å¢ƒè®¾ç½®)
3. [æœ¬åœ°å¼€å‘è¿è¡Œ](#æœ¬åœ°å¼€å‘è¿è¡Œ)
4. [Dockeréƒ¨ç½²](#dockeréƒ¨ç½²)
5. [è®­ç»ƒæ¨¡å‹](#è®­ç»ƒæ¨¡å‹)
6. [è¿è¡Œæµ‹è¯•](#è¿è¡Œæµ‹è¯•)
7. [APIä½¿ç”¨ç¤ºä¾‹](#apiä½¿ç”¨ç¤ºä¾‹)
8. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## å¿«é€Ÿå¼€å§‹

### æœ€ç®€å•çš„æ–¹å¼ (3æ­¥)

```bash
# 1. è§£å‹é¡¹ç›®
unzip media-recommender.zip
cd media-recommender

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 3. è¿è¡ŒAPIæœåŠ¡
python -m uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000
```

æ‰“å¼€æµè§ˆå™¨è®¿é—®: http://localhost:8000/docs æŸ¥çœ‹APIæ–‡æ¡£

---

## ç¯å¢ƒè®¾ç½®

### ç³»ç»Ÿè¦æ±‚
- Python 3.9+
- CUDA 11.8+ (GPUè®­ç»ƒï¼Œå¯é€‰)
- Docker & Docker Compose (å®¹å™¨éƒ¨ç½²ï¼Œå¯é€‰)
- 8GB+ RAM (æ¨è16GB)

### æ–¹æ³•1: ä½¿ç”¨pip

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install --upgrade pip
pip install -r requirements.txt

# å®‰è£…é¡¹ç›®ï¼ˆå¼€å‘æ¨¡å¼ï¼‰
pip install -e .

# ä¸‹è½½spaCyæ¨¡å‹
python -m spacy download en_core_web_sm
```

### æ–¹æ³•2: ä½¿ç”¨conda

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n media-rec python=3.10
conda activate media-rec

# å®‰è£…PyTorch (é€‰æ‹©é€‚åˆä½ CUDAç‰ˆæœ¬çš„)
conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia
# æˆ– CPUç‰ˆæœ¬
conda install pytorch torchvision cpuonly -c pytorch

# å®‰è£…å…¶ä»–ä¾èµ–
pip install -r requirements.txt
pip install -e .
```

### æ–¹æ³•3: ä½¿ç”¨Makefile

```bash
# å®‰è£…å¼€å‘ä¾èµ–
make dev-install

# æˆ–åªå®‰è£…è¿è¡Œæ—¶ä¾èµ–
make install
```

---

## æœ¬åœ°å¼€å‘è¿è¡Œ

### 1. é…ç½®ç¯å¢ƒå˜é‡

```bash
# å¤åˆ¶ç¯å¢ƒå˜é‡æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘é…ç½® (å¯é€‰ï¼Œä½¿ç”¨é»˜è®¤å€¼ä¹Ÿå¯ä»¥è¿è¡Œ)
nano .env
```

`.env` ä¸»è¦é…ç½®é¡¹:
```env
# APIé…ç½®
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=true

# æ¨¡å‹é…ç½®
MODEL_CHECKPOINT_DIR=./checkpoints
BERT_MODEL_NAME=bert-base-uncased

# æ•°æ®åº“ (æœ¬åœ°å¼€å‘å¯è·³è¿‡)
DATABASE_URL=postgresql://user:pass@localhost:5432/recommender
REDIS_URL=redis://localhost:6379/0
```

### 2. å¯åŠ¨APIæœåŠ¡

```bash
# å¼€å‘æ¨¡å¼ (è‡ªåŠ¨é‡è½½)
python -m uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000

# æˆ–ä½¿ç”¨Makefile
make serve

# ç”Ÿäº§æ¨¡å¼ (å¤šworker)
make serve-prod
```

### 3. éªŒè¯æœåŠ¡è¿è¡Œ

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# åº”è¯¥è¿”å›:
# {"status": "healthy", "model_loaded": true, ...}
```

### 4. è®¿é—®APIæ–‡æ¡£

- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

---

## Dockeréƒ¨ç½²

### æ–¹æ³•1: åªè¿è¡ŒAPIæœåŠ¡

```bash
# æ„å»ºé•œåƒ
docker build -t media-recommender:latest .

# è¿è¡Œå®¹å™¨
docker run -d \
  --name recommender-api \
  -p 8000:8000 \
  -e DEBUG=false \
  media-recommender:latest
```

### æ–¹æ³•2: å®Œæ•´æœåŠ¡æ ˆ (æ¨è)

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡ (API + PostgreSQL + Redis + Elasticsearch + ç›‘æ§)
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f api

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps
```

æœåŠ¡ç«¯å£:
| æœåŠ¡ | ç«¯å£ | è¯´æ˜ |
|------|------|------|
| API | 8000 | æ¨èæœåŠ¡ |
| PostgreSQL | 5432 | æ•°æ®åº“ |
| Redis | 6379 | ç¼“å­˜ |
| Elasticsearch | 9200 | æœç´¢ |
| Prometheus | 9090 | ç›‘æ§æŒ‡æ ‡ |
| Grafana | 3000 | ç›‘æ§é¢æ¿ |
| MLflow | 5001 | å®éªŒè¿½è¸ª |
| Jaeger | 16686 | åˆ†å¸ƒå¼è¿½è¸ª |

### æ–¹æ³•3: å¼€å‘ç¯å¢ƒDocker

```bash
# ä½¿ç”¨å¼€å‘é…ç½®
docker-compose -f docker-compose.yml up -d postgres redis

# APIåœ¨æœ¬åœ°è¿è¡Œï¼Œæ•°æ®åº“ç”¨Docker
python -m uvicorn src.api.main:app --reload
```

---

## è®­ç»ƒæ¨¡å‹

### 1. ä½¿ç”¨ç»¼åˆè®­ç»ƒè„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# è®­ç»ƒæ‰€æœ‰æ¨¡å‹
python scripts/train_all.py

# è®­ç»ƒç‰¹å®šæ¨¡å‹
python scripts/train_all.py --models ncf,bert4rec --epochs 20

# ä½¿ç”¨GPUè®­ç»ƒ
python scripts/train_all.py --device cuda --data-size large

# è®­ç»ƒåè¯„ä¼°
python scripts/train_all.py --epochs 10 --eval
```

### 2. è®­ç»ƒå•ä¸ªæ¨¡å‹

```bash
# ååŒè¿‡æ»¤
python -m src.training.train_collaborative --epochs 10

# åºåˆ—æ¨¡å‹
python -m src.training.train_sequential --model lstm --epochs 10
python -m src.training.train_sequential --model sasrec --epochs 10
```

### 3. å¿«é€ŸéªŒè¯Demo

```bash
# è®¾ç½®Pythonè·¯å¾„
export PYTHONPATH=.

# è¿è¡Œdemoè„šæœ¬
python scripts/demo.py
```

### 4. å®Œæ•´è®­ç»ƒç¤ºä¾‹

```python
# train_example.py
import torch
from torch.utils.data import DataLoader
from src.models import CollaborativeConfig, NeuralCollaborativeFiltering
from src.data import InteractionDataset, DataProcessor
from src.training import Trainer, TrainingConfig

# 1. åŠ è½½æ•°æ®
processor = DataProcessor()
train_data, val_data, test_data = processor.load_and_split('data/interactions.csv')

# 2. åˆ›å»ºDataset
train_dataset = InteractionDataset(train_data, processor.user_map, processor.item_map)
val_dataset = InteractionDataset(val_data, processor.user_map, processor.item_map)

train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=256)

# 3. åˆå§‹åŒ–æ¨¡å‹
config = CollaborativeConfig(
    num_users=len(processor.user_map),
    num_items=len(processor.item_map),
    embedding_dim=64
)
model = NeuralCollaborativeFiltering(config)

# 4. è®­ç»ƒ
train_config = TrainingConfig(
    epochs=50,
    learning_rate=0.001,
    early_stopping_patience=5
)

trainer = Trainer(model, train_config, device='cuda')
trainer.train(train_loader, val_loader)

# 5. ä¿å­˜æ¨¡å‹
torch.save(model.state_dict(), 'checkpoints/ncf_model.pt')
```

è¿è¡Œ:
```bash
python train_example.py
```

---

## è¿è¡Œæµ‹è¯•

### è¿è¡Œæ‰€æœ‰æµ‹è¯•

```bash
# ä½¿ç”¨pytest
pytest tests/ -v

# æˆ–ä½¿ç”¨Makefile
make test
```

### è¿è¡Œç‰¹å®šæµ‹è¯•

```bash
# åªæµ‹è¯•æ¨¡å‹
pytest tests/test_models.py -v

# åªæµ‹è¯•API
pytest tests/test_api.py -v

# åªæµ‹è¯•æ•°æ®å¤„ç†
pytest tests/test_data.py -v
```

### æµ‹è¯•è¦†ç›–ç‡

```bash
# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest tests/ --cov=src --cov-report=html

# æŸ¥çœ‹æŠ¥å‘Š
open htmlcov/index.html
```

### å¿«é€ŸéªŒè¯æ¨¡å‹

```python
# quick_test.py
import torch
from src.models import (
    CollaborativeConfig, NeuralCollaborativeFiltering,
    TransformerRecConfig, BERT4Rec,
    GNNConfig, LightGCN
)

print("Testing models...")

# Test NCF
config = CollaborativeConfig(num_users=100, num_items=500)
ncf = NeuralCollaborativeFiltering(config)
users = torch.randint(0, 100, (32,))
items = torch.randint(0, 500, (32,))
scores = ncf(users, items)
print(f"âœ“ NCF output: {scores.shape}")

# Test BERT4Rec
config = TransformerRecConfig(num_items=500, max_seq_len=50)
bert4rec = BERT4Rec(config)
sequences = torch.randint(0, 500, (4, 50))
logits = bert4rec(sequences)
print(f"âœ“ BERT4Rec output: {logits.shape}")

# Test LightGCN
config = GNNConfig(num_users=100, num_items=500)
lightgcn = LightGCN(config)
print(f"âœ“ LightGCN initialized")

print("\nâœ… All models working correctly!")
```

è¿è¡Œ:
```bash
python quick_test.py
```

---

## APIä½¿ç”¨ç¤ºä¾‹

### 1. è·å–æ¨è

```bash
# è·å–ç”¨æˆ·æ¨è
curl -X POST "http://localhost:8000/api/v1/recommendations/123" \
  -H "Content-Type: application/json" \
  -d '{
    "n_recommendations": 10,
    "exclude_items": [1, 2, 3]
  }'
```

å“åº”:
```json
{
  "user_id": "123",
  "recommendations": [
    {"item_id": 456, "score": 0.95, "source": "hybrid"},
    {"item_id": 789, "score": 0.92, "source": "collaborative"}
  ],
  "generated_at": "2024-01-15T10:30:00Z"
}
```

### 2. è®°å½•ç”¨æˆ·äº¤äº’

```bash
curl -X POST "http://localhost:8000/api/v1/interactions" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "123",
    "item_id": 456,
    "interaction_type": "click",
    "timestamp": "2024-01-15T10:30:00Z"
  }'
```

### 3. è·å–ç›¸ä¼¼ç‰©å“

```bash
curl "http://localhost:8000/api/v1/items/456/similar?n=5"
```

### 4. Pythonå®¢æˆ·ç«¯ç¤ºä¾‹

```python
import requests

class RecommenderClient:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
    
    def get_recommendations(self, user_id, n=10):
        response = requests.post(
            f"{self.base_url}/api/v1/recommendations/{user_id}",
            json={"n_recommendations": n}
        )
        return response.json()
    
    def log_interaction(self, user_id, item_id, interaction_type="click"):
        response = requests.post(
            f"{self.base_url}/api/v1/interactions",
            json={
                "user_id": user_id,
                "item_id": item_id,
                "interaction_type": interaction_type
            }
        )
        return response.json()

# ä½¿ç”¨
client = RecommenderClient()
recs = client.get_recommendations(user_id="123", n=10)
print(recs)
```

---

## å¸¸è§é—®é¢˜

### Q1: ç¼ºå°‘ä¾èµ–åŒ…

```bash
# å®‰è£…ç¼ºå¤±çš„åŒ…
pip install <package_name>

# æˆ–é‡æ–°å®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r requirements.txt --force-reinstall
```

### Q2: CUDA/GPUé—®é¢˜

```bash
# æ£€æŸ¥PyTorchæ˜¯å¦æ£€æµ‹åˆ°GPU
python -c "import torch; print(torch.cuda.is_available())"

# å¦‚æœè¿”å›Falseï¼Œå®‰è£…CUDAç‰ˆPyTorch
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### Q3: å†…å­˜ä¸è¶³

```python
# åœ¨è®­ç»ƒæ—¶å‡å°‘batch_size
train_config = TrainingConfig(batch_size=64)  # é»˜è®¤256

# æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
train_config = TrainingConfig(
    batch_size=64,
    gradient_accumulation_steps=4
)
```

### Q4: ç«¯å£è¢«å ç”¨

```bash
# æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
lsof -i :8000

# ä½¿ç”¨å…¶ä»–ç«¯å£
python -m uvicorn src.api.main:app --port 8001
```

### Q5: Dockeræ„å»ºå¤±è´¥

```bash
# æ¸…ç†Dockerç¼“å­˜
docker system prune -a

# é‡æ–°æ„å»º
docker-compose build --no-cache
```

### Q6: æ¨¡å‹åŠ è½½å¤±è´¥

```python
# æ£€æŸ¥checkpointè·¯å¾„
import os
print(os.path.exists('checkpoints/model.pt'))

# ä½¿ç”¨CPUåŠ è½½GPUè®­ç»ƒçš„æ¨¡å‹
model.load_state_dict(
    torch.load('model.pt', map_location='cpu')
)
```

---

## é¡¹ç›®ç»“æ„é€Ÿè§ˆ

```
media-recommender/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/          # 50+ç§MLæ¨¡å‹
â”‚   â”œâ”€â”€ data/            # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ training/        # è®­ç»ƒé€»è¾‘
â”‚   â”œâ”€â”€ api/             # FastAPIæœåŠ¡
â”‚   â””â”€â”€ utils/           # å·¥å…·å‡½æ•°
â”œâ”€â”€ tests/               # æµ‹è¯•æ–‡ä»¶
â”œâ”€â”€ configs/             # é…ç½®æ–‡ä»¶
â”œâ”€â”€ docker/              # Dockeré…ç½®
â”œâ”€â”€ scripts/             # éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ requirements.txt     # ä¾èµ–
â”œâ”€â”€ Dockerfile          
â”œâ”€â”€ docker-compose.yml   
â””â”€â”€ Makefile            # å¸¸ç”¨å‘½ä»¤
```

---

## ä¸‹ä¸€æ­¥

1. **æœ¬åœ°æµ‹è¯•**: `make test`
2. **å¯åŠ¨æœåŠ¡**: `make serve`
3. **Dockeréƒ¨ç½²**: `docker-compose up -d`
4. **è®­ç»ƒè‡ªå·±çš„æ¨¡å‹**: å‡†å¤‡æ•°æ®ï¼Œè¿è¡Œè®­ç»ƒè„šæœ¬
5. **æŸ¥çœ‹ç›‘æ§**: http://localhost:3000 (Grafana)

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—:
```bash
# APIæ—¥å¿—
docker-compose logs -f api

# æˆ–æœ¬åœ°è¿è¡Œæ—¶æŸ¥çœ‹ç»ˆç«¯è¾“å‡º
```
